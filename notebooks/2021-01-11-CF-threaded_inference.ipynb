{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, ToTensor, RandomCrop, RandomRotation, Normalize\n",
    "\n",
    "from repath.utils.paths import project_root\n",
    "import repath.data.datasets.camelyon16 as camelyon16\n",
    "from repath.preprocess.tissue_detection import TissueDetectorOTSU\n",
    "from repath.preprocess.patching import GridPatchFinder, SlidesIndex, SlidesIndexResults\n",
    "from repath.preprocess.sampling import split_camelyon16, balanced_sample\n",
    "from torchvision.models import GoogLeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"wang\"\n",
    "experiment_root = project_root() / \"experiments\" / experiment_name\n",
    "tissue_detector = TissueDetectorOTSU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchClassifier(pl.LightningModule):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.model = GoogLeNet(num_classes=2)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output, aux2, aux1 = self.model(x)\n",
    "        pred = torch.log_softmax(output, dim=1)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss1 = criterion(output, y)\n",
    "        loss2 = criterion(aux1, y)\n",
    "        loss3 = criterion(aux2, y)\n",
    "        loss = loss1 + 0.3 * loss2 + 0.3 * loss3\n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "        correct=pred.argmax(dim=1).eq(y).sum().item()\n",
    "        total=len(y)   \n",
    "        accu = correct / total\n",
    "        self.log(\"train_accuracy\", accu)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output = self.model(x)\n",
    "        pred = torch.log_softmax(output, dim=1)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(output, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        \n",
    "        correct=pred.argmax(dim=1).eq(y).sum().item()\n",
    "        total=len(y)   \n",
    "        accu = correct / total\n",
    "        self.log(\"val_accuracy\", accu)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.model.parameters(), \n",
    "                                    lr=0.01, \n",
    "                                    momentum=0.9, \n",
    "                                    weight_decay=0.0005)\n",
    "        scheduler = {\n",
    "            'scheduler': torch.optim.lr_scheduler.StepLR(optimizer, step_size=50000, gamma=0.5),\n",
    "            'interval': 'step' \n",
    "        }\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from repath.postprocess.slide_dataset import SlideDataset\n",
    "\n",
    "def evaluate_loop_dp(model, device, loader, num_classes):\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    num_samples = len(loader) * loader.batch_size\n",
    "\n",
    "    prob_out = np.zeros((num_samples, num_classes))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(loader):\n",
    "            data, target = batch\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            sm = torch.nn.Softmax(1)\n",
    "            output_sm = sm(output)\n",
    "            pred_prob = output_sm.cpu().numpy()  # rows: batch_size, cols: num_classes\n",
    "\n",
    "            start = idx * loader.batch_size\n",
    "            end = start + pred_prob.shape[0]\n",
    "            prob_out[start:end, :] = pred_prob\n",
    "\n",
    "            if idx % 100 == 0:\n",
    "                print('Batch {} of {}'.format(idx, len(loader)))\n",
    "\n",
    "    return prob_out\n",
    "\n",
    "\n",
    "def evaluate_loop_threaded(model, device, loader, num_classes):\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    num_samples = len(loader) * loader.batch_size\n",
    "\n",
    "    prob_out = np.zeros((num_samples, num_classes))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(loader):\n",
    "            data, target = batch\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            sm = torch.nn.Softmax(1)\n",
    "            output_sm = sm(output)\n",
    "            pred_prob = output_sm.cpu().numpy()  # rows: batch_size, cols: num_classes\n",
    "\n",
    "            start = idx * loader.batch_size\n",
    "            end = start + pred_prob.shape[0]\n",
    "            prob_out[start:end, :] = pred_prob\n",
    "\n",
    "            if idx % 100 == 0:\n",
    "                print('Batch {} of {}'.format(idx, len(loader)))\n",
    "\n",
    "    return prob_out\n",
    "\n",
    "\n",
    "\n",
    "def inference_on_slide(slideps: 'SlidePatchSet', model: torch.nn.Module, num_classes: int,\n",
    "                       batch_size: int, num_workers: int, transform) -> np.array:\n",
    "\n",
    "    \"\"\" runs inference for every patch on a slide using data parallel\n",
    "\n",
    "    Outputs probabilities for each class\n",
    "\n",
    "    Args:\n",
    "        slideps: A SlidePatchSet object containing all non background patches for the slide\n",
    "        model: a patch classifier model\n",
    "        num_classes: the number of output classes predicted by the model\n",
    "        batch_size: the batch size for inference\n",
    "        num_workers: the num_workers for inference\n",
    "        ntransforms: the number of predictions per patch. Each patch can be predicted multiple times eg rotations\n",
    "            or flips, the mean across thes transforms is found for each patch\n",
    "\n",
    "    Returns:\n",
    "        An ndarray the same length as the slide dataset with a column for each class containing a float that\n",
    "        represents the probability of the patch being that class.\n",
    "    \"\"\"    \n",
    "\n",
    "\n",
    "    # Check if GPU is available\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    slide_dataset = SlideDataset(slideps, transform)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(slide_dataset, shuffle=False,\n",
    "                                              batch_size=batch_size,  num_workers=num_workers)\n",
    "\n",
    "    probabilities = evaluate_loop_dp(model, device, test_loader, num_classes)\n",
    "\n",
    "    ### HACK - ntransforms only needed for google paper need to sort out using transform compose or list of transform compose\n",
    "    ntransforms = 1\n",
    "\n",
    "    npreds = int(len(slide_dataset) * ntransforms)\n",
    "\n",
    "    probabilities = probabilities[0:npreds, :]\n",
    "\n",
    "    if ntransforms > 1:\n",
    "        prob_rows = probabilities.shape[0]\n",
    "        prob_rows = int(prob_rows / ntransforms)\n",
    "        probabilities_reshape = np.empty((prob_rows, num_classes))\n",
    "        for cl in num_classes:\n",
    "            class_probs = probabilities[:, cl]\n",
    "            class_probs = np.reshape(class_probs, (ntransforms, prob_rows)).T\n",
    "            class_probs = np.mean(class_probs, axis=1)\n",
    "            probabilities_reshape[:, cl] = class_probs\n",
    "        probabilities = probabilities_reshape\n",
    "\n",
    "    return probabilities\n",
    "\n",
    "\n",
    "def inference_on_slide_threaded(slideps: 'SlidePatchSet', model: torch.nn.Module, num_classes: int,\n",
    "                       batch_size: int, num_workers: int, transform, device) -> np.array:\n",
    "\n",
    "    \"\"\" runs inference for every patch on a slide using data parallel\n",
    "\n",
    "    Outputs probabilities for each class\n",
    "\n",
    "    Args:\n",
    "        slideps: A SlidePatchSet object containing all non background patches for the slide\n",
    "        model: a patch classifier model\n",
    "        num_classes: the number of output classes predicted by the model\n",
    "        batch_size: the batch size for inference\n",
    "        num_workers: the num_workers for inference\n",
    "        ntransforms: the number of predictions per patch. Each patch can be predicted multiple times eg rotations\n",
    "            or flips, the mean across thes transforms is found for each patch\n",
    "\n",
    "    Returns:\n",
    "        An ndarray the same length as the slide dataset with a column for each class containing a float that\n",
    "        represents the probability of the patch being that class.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # Check if GPU is available\n",
    "    device = torch.device(f\"cuda:{device}\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    slide_dataset = SlideDataset(slideps, transform)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(slide_dataset, shuffle=False,\n",
    "                                              batch_size=batch_size,  num_workers=num_workers)\n",
    "\n",
    "    probabilities = evaluate_loop_threaded(model, device, test_loader, num_classes)\n",
    "\n",
    "    ### HACK - ntransforms only needed for google paper need to sort out using transform compose or list of transform compose\n",
    "    ntransforms = 1\n",
    "\n",
    "    npreds = int(len(slide_dataset) * ntransforms)\n",
    "\n",
    "    probabilities = probabilities[0:npreds, :]\n",
    "\n",
    "    if ntransforms > 1:\n",
    "        prob_rows = probabilities.shape[0]\n",
    "        prob_rows = int(prob_rows / ntransforms)\n",
    "        probabilities_reshape = np.empty((prob_rows, num_classes))\n",
    "        for cl in num_classes:\n",
    "            class_probs = probabilities[:, cl]\n",
    "            class_probs = np.reshape(class_probs, (ntransforms, prob_rows)).T\n",
    "            class_probs = np.mean(class_probs, axis=1)\n",
    "            probabilities_reshape[:, cl] = class_probs\n",
    "        probabilities = probabilities_reshape\n",
    "\n",
    "    return probabilities\n",
    "\n",
    "\n",
    "\n",
    "### Below is initial pseudo code on ddp needs developing to speed up inference\n",
    "#def process_predict(rank, loader_for_subset, model, batch_size, num_classes):\n",
    "#    model = model.to_gpu(rank)\n",
    "#    output = Tensor.empty(len(loader_for_subset) * batch_size, num_classes))\n",
    "#    for batch_idx, batch in enumerate(loader_for_subset):\n",
    "#        batch = batch.to_gpu(rank)\n",
    "#        y = model(batch)\n",
    "#        y = nn.softmax(y)\n",
    "#        output[batch_idx * batch_size, :] = y\n",
    "#    return output\n",
    "#\n",
    "#\n",
    "#torch.multiprocessing.spawn(fn, args=(), nprocs=1, join=True, daemon=False, start_method='spawn')#\n",
    "#\n",
    "#\n",
    "#def distibuted_predict(model, patchset):\n",
    "#    dataset = SlideDataset(patchset)\n",
    "#    sampler = SequentialSampler(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "import threading\n",
    "from typing import List, Sequence\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "from repath.data.datasets import Dataset\n",
    "from repath.preprocess.patching.patch_finder import PatchFinder\n",
    "from repath.preprocess.tissue_detection.tissue_detector import TissueDetector\n",
    "from repath.data.slides import Region\n",
    "from repath.utils.convert import remove_item_from_dict\n",
    "from repath.postprocess.prediction import inference_on_slide\n",
    "\n",
    "\n",
    "class PatchSet(Sequence):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        patch_size: int,\n",
    "        level: int,\n",
    "        patches_df: pd.DataFrame,\n",
    "    ) -> None:\n",
    "        self.dataset = dataset\n",
    "        self.patch_size = patch_size\n",
    "        self.level = level\n",
    "        self.patches_df = patches_df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patches_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.patches_df.iloc[idx,]\n",
    "\n",
    "    def summary(self) -> pd.DataFrame:\n",
    "        by_label = self.patches_df.groupby(\"label\").size()\n",
    "        labels = {v: k for k, v in self.dataset.labels.items()}\n",
    "        count_df = by_label.to_frame().T.rename(columns = labels)\n",
    "        columns = list(labels.values())\n",
    "        summary = pd.DataFrame(columns=columns)\n",
    "        for l in labels.values():\n",
    "            if l in count_df:\n",
    "                summary[l] = count_df[l]\n",
    "            else:\n",
    "                summary[l] = 0\n",
    "        summary = summary.replace(np.nan, 0)  # if there are no patches for some classes\n",
    "        return summary\n",
    "\n",
    "\n",
    "class CombinedPatchSet(PatchSet):\n",
    "    def __init__(self, dataset: Dataset, patch_size: int, level: int, patches_df: pd.DataFrame) -> None:\n",
    "        super().__init__(dataset, patch_size, level, patches_df)\n",
    "        # columns of patches_df are x, y, label, slide_idx\n",
    "\n",
    "    def save_patches(self, output_dir: Path, transforms: List[transforms.Compose] = None) -> None:\n",
    "        for slide_idx, group in self.patches_df.groupby('slide_idx'):\n",
    "            slide_path, _, _, _ = self.dataset[slide_idx]\n",
    "            with self.dataset.slide_cls(slide_path) as slide:\n",
    "                print(f\"Writing patches for {self.dataset.to_rel_path(slide_path)}\")\n",
    "                for row in group.itertuples():\n",
    "                    # read the patch image from the slide\n",
    "                    region = Region.patch(row.x, row.y, self.patch_size, self.level)\n",
    "                    image = slide.read_region(region)\n",
    "\n",
    "                    # apply any transforms, as indexed in the 'transform' column\n",
    "                    if transforms:\n",
    "                        image = transforms[row.transform](image)\n",
    "\n",
    "                    # get the patch label as a string\n",
    "                    labels = {v: k for k, v in self.dataset.labels.items()}\n",
    "                    label = labels[row.label]\n",
    "\n",
    "                    # ensure the output directory exists\n",
    "                    output_subdir = output_dir / label\n",
    "                    output_subdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                    # write out the slide\n",
    "                    rel_slide_path = self.dataset.to_rel_path(slide_path)\n",
    "                    slide_name_str = str(rel_slide_path)[:-4].replace('/', '-')\n",
    "                    patch_filename = slide_name_str + f\"-{row.x}-{row.y}.png\"\n",
    "                    image_path = output_dir / label / patch_filename\n",
    "                    image.save(image_path)\n",
    "\n",
    "\n",
    "class CombinedIndex(object):\n",
    "    def __init__(self, cps: List[CombinedPatchSet]) -> None:\n",
    "        self.datasets = [cp.dataset for cp in cps]\n",
    "        self.patchsizes = [cp.patch_size for cp in cps]\n",
    "        self.levels = [cp.level for cp in cps]\n",
    "        patches_dfs = [cp.patches_df for cp in cps]\n",
    "        patches_df = pd.concat(patches_dfs, axis=0)\n",
    "        cps_index = [[idx] * len(cp) for idx, cp in enumerate(cps)]\n",
    "        cps_index = [item for sublist in cps_index for item in sublist]\n",
    "        patches_df['cps_idx'] = cps_index\n",
    "        self.patches_df = patches_df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patches_df)\n",
    "\n",
    "    @classmethod\n",
    "    def for_slide_indexes(cls, indexes: List['SlidesIndex']) -> 'CombinedIndex':\n",
    "        cps = [index.as_combined() for index in indexes]\n",
    "        ci = cls(cps)\n",
    "        return ci\n",
    "\n",
    "    def save_patches(self, output_dir: Path, transforms: List[transforms.Compose] = None, affix: str = '') -> None:\n",
    "        for cps_idx, cps_group in self.patches_df.groupby('cps_idx'):\n",
    "            for slide_idx, sl_group in cps_group.groupby('slide_idx'):\n",
    "                slide_path, _, _, _ = self.datasets[cps_idx][slide_idx]\n",
    "                with self.datasets[cps_idx].slide_cls(slide_path) as slide:\n",
    "                    print(f\"Writing patches for {self.datasets[cps_idx].to_rel_path(slide_path)}\")\n",
    "                    for row in sl_group.itertuples():\n",
    "                        # read the patch image from the slide\n",
    "                        region = Region.patch(row.x, row.y, self.patchsizes[cps_idx], self.levels[cps_idx])\n",
    "                        image = slide.read_region(region)\n",
    "\n",
    "                        # apply any transforms, as indexed in the 'transform' column\n",
    "                        if transforms:\n",
    "                            image = transforms[row.transform](image)\n",
    "\n",
    "                        # get the patch label as a string\n",
    "                        labels = {v: k for k, v in self.datasets[cps_idx].labels.items()}\n",
    "                        label = labels[row.label]\n",
    "\n",
    "                        # ensure the output directory exists\n",
    "                        output_subdir = output_dir / label\n",
    "                        output_subdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                        # write out the slide\n",
    "                        rel_slide_path = self.datasets[cps_idx].to_rel_path(slide_path)\n",
    "                        slide_name_str = str(rel_slide_path)[:-4].replace('/', '-')\n",
    "                        patch_filename = slide_name_str + f\"-{row.x}-{row.y}{affix}.png\"\n",
    "                        image_path = output_dir / label / patch_filename\n",
    "                        image.save(image_path)\n",
    "\n",
    "\n",
    "\n",
    "class SlidePatchSet(PatchSet):\n",
    "    def __init__(\n",
    "        self, \n",
    "        slide_idx: int,\n",
    "        dataset: Dataset,\n",
    "        patch_size: int,\n",
    "        level: int,\n",
    "        patches_df: pd.DataFrame\n",
    "    ) -> None:\n",
    "        super().__init__(dataset, patch_size, level, patches_df)\n",
    "        self.slide_idx = slide_idx\n",
    "        abs_slide_path, self.annotation_path, self.label, tags = dataset[slide_idx]\n",
    "        self.slide_path = dataset.to_rel_path(abs_slide_path)\n",
    "        self.tags = [tg.strip() for tg in tags.split(';')]\n",
    "\n",
    "    @classmethod\n",
    "    def index_slide(cls, slide_idx: int, dataset: Dataset, tissue_detector: TissueDetector, patch_finder: PatchFinder):\n",
    "        slide_path, annotation_path, _, _ = dataset[slide_idx]\n",
    "        with dataset.slide_cls(slide_path) as slide:\n",
    "            print(f\"indexing {slide_path.name}\")  # TODO: Add proper logging!\n",
    "            annotations = dataset.load_annotations(annotation_path)\n",
    "            labels_shape = slide.dimensions[patch_finder.labels_level].as_shape()\n",
    "            scale_factor = 2 ** patch_finder.labels_level\n",
    "            labels_image = annotations.render(labels_shape, scale_factor)\n",
    "            tissue_mask = tissue_detector(slide.get_thumbnail(patch_finder.labels_level))\n",
    "            labels_image[~tissue_mask] = 0\n",
    "            df, level, size = patch_finder(labels_image, slide.dimensions[patch_finder.patch_level])\n",
    "            patchset = cls(slide_idx, dataset, size, level, df)\n",
    "            return patchset\n",
    "\n",
    "    @property\n",
    "    def abs_slide_path(self):\n",
    "        return self.dataset.to_abs_path(self.slide_path)\n",
    "\n",
    "    def open_slide(self):\n",
    "        return self.dataset.slide_cls(self.abs_slide_path)\n",
    "\n",
    "\n",
    "class SlidesIndex(Sequence):\n",
    "    def __init__(self, dataset: Dataset, patches: List[SlidePatchSet]) -> None:\n",
    "        self.dataset = dataset\n",
    "        self.patches = patches\n",
    "\n",
    "    @classmethod\n",
    "    def index_dataset(cls, dataset: Dataset, tissue_detector: TissueDetector, patch_finder: PatchFinder) -> 'SlidesIndex':\n",
    "        patchsets = [SlidePatchSet.index_slide(idx, dataset, tissue_detector, patch_finder) for idx in range(len(dataset))]\n",
    "        return cls(dataset, patchsets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patches)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.patches[idx]\n",
    "\n",
    "    @property\n",
    "    def patch_size(self):\n",
    "        return self.patches[0].patch_size\n",
    "\n",
    "    @property\n",
    "    def level(self):\n",
    "        return self.patches[0].level\n",
    "\n",
    "    def summary(self) -> pd.DataFrame:\n",
    "        summaries = [p.summary() for p in self.patches]\n",
    "        slide_path = [p.slide_path for p in self.patches]\n",
    "        slide_label = [p.label for p in self.patches]\n",
    "        rtn = pd.concat(summaries)\n",
    "        rtn['slide_path'] = slide_path\n",
    "        rtn['slide_label'] = slide_label\n",
    "        rtn = rtn.reset_index()\n",
    "        rtn = rtn.drop('index', axis=1)\n",
    "        rtn = rtn[['slide_path', 'slide_label'] + list(self.dataset.labels.keys())]\n",
    "        return rtn\n",
    "\n",
    "    def as_combined(self) -> CombinedPatchSet:\n",
    "        # combine all patchsets into one\n",
    "        frames = [ps.patches_df for ps in self.patches]\n",
    "        slide_indexes = [[ps.slide_idx]*len(ps) for ps in self.patches]\n",
    "        slide_indexes = list(chain(*slide_indexes))\n",
    "        patches_df =  pd.concat(frames, axis=0)\n",
    "        patches_df['slide_idx'] = slide_indexes\n",
    "        return CombinedPatchSet(self.dataset, self.patch_size, self.level, patches_df)\n",
    "\n",
    "    def save(self, output_dir: Path) -> None:\n",
    "        columns = ['slide_idx', 'csv_path', 'level', 'patch_size']\n",
    "        index_df = pd.DataFrame(columns=columns)\n",
    "        for ps in self.patches:\n",
    "            # save out the csv file for this slide\n",
    "            csv_path = ps.slide_path.with_suffix('.csv')\n",
    "            csv_path = output_dir / csv_path\n",
    "            csv_path.parents[0].mkdir(parents=True, exist_ok=True)\n",
    "            print(f\"Saving {csv_path}\")\n",
    "            ps.patches_df.to_csv(csv_path, index=False)\n",
    "\n",
    "            # append information about slide to index\n",
    "            info = np.array([ps.slide_idx, csv_path, ps.level, ps.patch_size])\n",
    "            info = np.reshape(info, (1, 4))\n",
    "            row = pd.DataFrame(info, columns=columns)\n",
    "            index_df = index_df.append(row, ignore_index=True)\n",
    "\n",
    "        # tidy up a bit and save the csv\n",
    "        index_df = index_df.astype({\"level\": int, \"patch_size\": int})\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        index_df.to_csv(output_dir / 'index.csv', index=False)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, dataset: Dataset, input_dir: Path) -> 'SlidesIndex':\n",
    "        def patchset_from_row(r) -> PatchSet:\n",
    "            patches_df = pd.read_csv(input_dir / r.csv_path)\n",
    "            return SlidePatchSet(int(r.slide_idx), dataset, int(r.patch_size), \n",
    "                                 int(r.level), patches_df)\n",
    "\n",
    "        index = pd.read_csv(input_dir / 'index.csv')\n",
    "        patches = [patchset_from_row(r) for r in index.itertuples()]\n",
    "        rtn = cls(dataset, patches)\n",
    "        return rtn\n",
    "\n",
    "\n",
    "class SlidePatchSetResults(SlidePatchSet):\n",
    "    def __init__(self, slide_idx: int, dataset: Dataset, patch_size: int, level: int, patches_df: pd.DataFrame) -> None:\n",
    "        super().__init__(slide_idx, dataset, patch_size, level, patches_df)\n",
    "        abs_slide_path, self.annotation_path, self.label, self.tags = dataset[slide_idx]\n",
    "        self.slide_path = dataset.to_rel_path(abs_slide_path)\n",
    "\n",
    "    @classmethod\n",
    "    def predict_slide(cls, sps: SlidePatchSet, classifier: nn.Module, batch_size: int, nworkers: int,\n",
    "                      transform):\n",
    "\n",
    "        just_patch_classes = remove_item_from_dict(sps.dataset.labels, \"background\")\n",
    "        num_classes = len(just_patch_classes)\n",
    "        probs_out = inference_on_slide(sps, classifier, num_classes, batch_size, nworkers, transform)\n",
    "        probs_df = pd.DataFrame(probs_out, columns=list(just_patch_classes.keys()))\n",
    "        probs_df = pd.concat((sps.patches_df, probs_df), axis=1)\n",
    "        patchsetresults = cls(sps.slide_idx, sps.dataset, sps.patch_size, sps.level, probs_df)\n",
    "        return patchsetresults\n",
    "\n",
    "    def to_heatmap(self, class_name: str) -> np.array:\n",
    "        self.patches_df.columns = [colname.lower() for colname in self.patches_df.columns]\n",
    "        class_name = class_name.lower()\n",
    "\n",
    "        self.patches_df['column'] = np.divide(self.patches_df.x, self.patch_size)\n",
    "        self.patches_df['row'] = np.divide(self.patches_df.y, self.patch_size)\n",
    "\n",
    "        max_rows = int(np.max(self.patches_df.row)) + 1\n",
    "        max_cols = int(np.max(self.patches_df.column)) + 1\n",
    "\n",
    "        # create a blank thumbnail\n",
    "        thumbnail_out = np.zeros((max_rows, max_cols))\n",
    "\n",
    "        # for each row in dataframe set the value of the pixel specified by row and column to the probability in clazz\n",
    "        for rw in range(len(self)):\n",
    "            df_row = self.patches_df.iloc[rw]\n",
    "            thumbnail_out[int(df_row.row), int(df_row.column)] = df_row[class_name]\n",
    "\n",
    "        return thumbnail_out\n",
    "\n",
    "    def save_csv(self, output_dir):\n",
    "        # save out the patches csv file for this slide\n",
    "        csv_path = output_dir / self.slide_path.with_suffix('.csv')\n",
    "        self.patches_df.to_csv(csv_path, index=False)\n",
    "\n",
    "    def save_heatmap(self, class_name: str, output_dir: Path):\n",
    "        # get the heatmap filename for this slide\n",
    "        img_path = output_dir / self.slide_path.with_suffix('.png')\n",
    "        # create heatmap and write out\n",
    "        heatmap = self.to_heatmap(class_name)\n",
    "        heatmap_out = np.array(np.multiply(heatmap, 255), dtype=np.uint8)\n",
    "        cv2.imwrite(str(img_path), heatmap_out)\n",
    "\n",
    "    @classmethod\n",
    "    def predict_slide_threaded(cls, sps: SlidePatchSet, classifier: nn.Module, batch_size: int, nworkers: int,\n",
    "                      transform, device: int):\n",
    "\n",
    "        just_patch_classes = remove_item_from_dict(sps.dataset.labels, \"background\")\n",
    "        num_classes = len(just_patch_classes)\n",
    "        probs_out = inference_on_slide_threaded(sps, classifier, num_classes, batch_size, nworkers, transform, device)\n",
    "        probs_df = pd.DataFrame(probs_out, columns=list(just_patch_classes.keys()))\n",
    "        probs_df = pd.concat((sps.patches_df, probs_df), axis=1)\n",
    "        patchsetresults = cls(sps.slide_idx, sps.dataset, sps.patch_size, sps.level, probs_df)\n",
    "        return patchsetresults\n",
    "\n",
    "\n",
    "class SlidesIndexResults(SlidesIndex):\n",
    "    def __init__(self, dataset: Dataset, patches: List[SlidePatchSet],\n",
    "                 output_dir: Path, results_dir_name: str, heatmap_dir_name: str) -> None:\n",
    "        super().__init__(dataset, patches)\n",
    "        self.output_dir = output_dir\n",
    "        self.results_dir_name = results_dir_name\n",
    "        self.heatmap_dir_name = heatmap_dir_name\n",
    "\n",
    "    @classmethod\n",
    "    def predict_dataset(cls,\n",
    "                        si: SlidesIndex,\n",
    "                        classifier: nn.Module,\n",
    "                        batch_size,\n",
    "                        num_workers,\n",
    "                        transform,\n",
    "                        output_dir: Path,\n",
    "                        results_dir_name: str,\n",
    "                        heatmap_dir_name: str) -> 'SlidesIndexResults':\n",
    "\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        results_dir = output_dir / results_dir_name\n",
    "        results_dir.mkdir(parents=True, exist_ok=True)\n",
    "        heatmap_dir = output_dir / heatmap_dir_name\n",
    "        heatmap_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        spsresults = []\n",
    "        for sps in si:\n",
    "            spsresult = SlidePatchSetResults.predict_slide(sps, classifier, batch_size, num_workers, transform)\n",
    "            print(f\"Saving {sps.slide_path}\")\n",
    "            results_slide_dir = results_dir / sps.slide_path.parents[0]\n",
    "            results_slide_dir.mkdir(parents=True, exist_ok=True)\n",
    "            spsresults.append(spsresult)\n",
    "            spsresult.save_csv(results_dir)\n",
    "            heatmap_slide_dir = heatmap_dir/ sps.slide_path.parents[0]\n",
    "            heatmap_slide_dir.mkdir(parents=True, exist_ok=True)\n",
    "            ### HACK since this is only binary at the moment it will always be the tumor heatmap we want need to change to work for multiple classes\n",
    "            spsresult.save_heatmap('tumor', heatmap_dir)\n",
    "\n",
    "        return cls(si.dataset, spsresults, output_dir, results_dir_name, heatmap_dir_name)\n",
    "\n",
    "    def save_results_index(self):\n",
    "        columns = ['slide_idx', 'csv_path', 'png_path', 'level', 'patch_size']\n",
    "        index_df = pd.DataFrame(columns=columns)\n",
    "        for ps in self.patches:\n",
    "            # save out the csv file for this slide\n",
    "            csv_path = self.output_dir / self.results_dir_name / ps.slide_path.with_suffix('.csv')\n",
    "            png_path = self.output_dir / self.heatmap_dir_name / ps.slide_path.with_suffix('.png')\n",
    "\n",
    "            # append information about slide to index\n",
    "            info = np.array([ps.slide_idx, csv_path, png_path, ps.level, ps.patch_size])\n",
    "            info = np.reshape(info, (1, 5))\n",
    "            row = pd.DataFrame(info, columns=columns)\n",
    "            index_df = index_df.append(row, ignore_index=True)\n",
    "\n",
    "        # tidy up a bit and save the csv\n",
    "        index_df = index_df.astype({\"level\": int, \"patch_size\": int})\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        index_df.to_csv(self.output_dir / 'results_index.csv', index=False)\n",
    "\n",
    "    @classmethod\n",
    "    def load_results_index(cls, dataset, input_dir, results_dir_name, heatmap_dir_name):\n",
    "        def patchset_from_row(r: namedtuple) -> SlidePatchSet:\n",
    "            patches_df = pd.read_csv(input_dir / r.csv_path)\n",
    "            return SlidePatchSetResults(int(r.slide_idx), dataset, int(r.patch_size),\n",
    "                                 int(r.level), patches_df)\n",
    "\n",
    "        index = pd.read_csv(input_dir / 'results_index.csv')\n",
    "        patches = [patchset_from_row(r) for r in index.itertuples()]\n",
    "        rtn = cls(dataset, patches, input_dir, results_dir_name, heatmap_dir_name)\n",
    "        return rtn\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def predict_dataset_threaded(cls,\n",
    "                        si: SlidesIndex,\n",
    "                        classifier: nn.Module,\n",
    "                        batch_size,\n",
    "                        num_workers,\n",
    "                        transform,\n",
    "                        output_dir: Path,\n",
    "                        results_dir_name: str,\n",
    "                        heatmap_dir_name: str) -> 'SlidesIndexResults':\n",
    "\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        results_dir = output_dir / results_dir_name\n",
    "        results_dir.mkdir(parents=True, exist_ok=True)\n",
    "        heatmap_dir = output_dir / heatmap_dir_name\n",
    "        heatmap_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        ### experiment to distribute slides across multigpus for inference\n",
    "        # find how many gpus\n",
    "        ngpus = torch.cuda.device_count()\n",
    "\n",
    "        # work out how many slides and numbers in each split\n",
    "        nslides = len(si)\n",
    "        splits = np.rint(np.linspace(0, nslides, num=(ngpus+1))).astype(int)\n",
    "        start_indexes = splits[0:ngpus]\n",
    "        end_indexes = splits[1:]\n",
    "        print(\"splits:\", splits)\n",
    "\n",
    "        # shuffle slide index\n",
    "        si = shuffle(si)\n",
    "        si_per_gpu = []\n",
    "        for ii in range(ngpus):\n",
    "            print(start_indexes[ii], end_indexes[ii])\n",
    "            si_gpu = si[start_indexes[ii]:end_indexes[ii]]\n",
    "            si_per_gpu.append(si_gpu)\n",
    "\n",
    "        def worker(num):\n",
    "            si_thread = si_per_gpu[num]\n",
    "            spsresults_thread = []\n",
    "            for sps in si_thread:\n",
    "                spsresult = SlidePatchSetResults.predict_slide_threaded(sps, classifier, batch_size, num_workers, transform, num)\n",
    "                print(f\"Saving {num}: {sps.slide_path}\")\n",
    "                results_slide_dir = results_dir / sps.slide_path.parents[0]\n",
    "                results_slide_dir.mkdir(parents=True, exist_ok=True)\n",
    "                spsresults_thread.append(spsresult)\n",
    "                spsresult.save_csv(results_dir)\n",
    "                heatmap_slide_dir = heatmap_dir/ sps.slide_path.parents[0]\n",
    "                heatmap_slide_dir.mkdir(parents=True, exist_ok=True)\n",
    "                ### HACK since this is only binary at the moment it will always be the tumor heatmap we want need to change to work for multiple classes\n",
    "                spsresult.save_heatmap('tumor', heatmap_dir)         \n",
    "            spsresults[num] = spsresults_thread\n",
    "            return \n",
    "\n",
    "        spsresults = [0] * ngpus\n",
    "        threads = []\n",
    "        for i in range(ngpus):\n",
    "            t = threading.Thread(target=worker, args=(i,))\n",
    "            threads.append(t)\n",
    "            t.start()\n",
    "            t.join()\n",
    "\n",
    "        spsresults_flat = [item for sublist in spsresults for item in sublist]\n",
    "\n",
    "\n",
    "        return cls(si.dataset, spsresults, output_dir, results_dir_name, heatmap_dir_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_path = list((experiment_root / \"patch_model\").glob(\"*.ckpt\"))[0]\n",
    "classifier = PatchClassifier.load_from_checkpoint(checkpoint_path=cp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir16 = experiment_root / \"train_index\" / \"pre_hnm_results\"\n",
    "\n",
    "results_dir_name = \"results\"\n",
    "heatmap_dir_name = \"heatmaps\"\n",
    "\n",
    "train16 = SlidesIndex.load(camelyon16.training(), experiment_root / \"train_index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([\n",
    "    RandomCrop((240, 240)),\n",
    "    ToTensor(),\n",
    "    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splits: [  0  27  54  81 108 134 161 188 215]\n",
      "0 27\n",
      "27 54\n",
      "54 81\n",
      "81 108\n",
      "108 134\n",
      "134 161\n",
      "161 188\n",
      "188 215\n"
     ]
    }
   ],
   "source": [
    "train_results16 = SlidesIndexResults.predict_dataset_threaded(train16, classifier, 128, 8, transform, output_dir16, results_dir_name, heatmap_dir_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results16.save_results_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "repath",
   "language": "python",
   "name": "repath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
